{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorBoard\n",
    "\n",
    "TensorBoard provides a suite of visualization tools to make it easier to understand, debug, and optimize Edward programs. You can use it \"to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it\"\n",
    "([tensorflow.org](https://www.tensorflow.org/get_started/summaries_and_tensorboard))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use TensorBoard, we need to specify a directory for storing logs during inference. For example, if manually controlling inference, call\n",
    "```python\n",
    "inference.initialize(logdir='log')\n",
    "```\n",
    "or if using the catch-all `inference.run()`, include `logdir` as an argument.\n",
    "As inference runs, files are outputted to `log/` within the working directory.\n",
    "In commandline, we run TensorBoard and point to that directory.\n",
    "```bash\n",
    "tensorboard --logdir=log/\n",
    "```\n",
    "The command will provide a web address to access TensorBoard.\n",
    "By default, it is http://localhost:6006.\n",
    "\n",
    "You're set up! To prettify TensorBoard, we need to ensure we have \n",
    "proper naming of the log directory and tensors in the computational graph.\n",
    "\n",
    "### Directory Naming\n",
    "\n",
    "The following variables are set when either calling `inference.initialize` or `inference.run`:\n",
    "\n",
    "+ `logdir`: The directory where all logs for inference are stored. For running inference and comparing across many hyperparameter configurations, we recommend a path such as `logdir='log/' + hyperparam_str`, where `hyperparam_str` is a string such as `batch_size_256_n_samples_5`.\n",
    "+ `log_timestamp` (bool): If True (which is by default), a UTC time-stamped subdirectory is placed in `logdir` in the format 'YYYYMMDD_HHMMSS\". This guarantees each run is stored in a unique subdirectory name per run so that TensorBoard can properly organize each run's information.\n",
    "       \n",
    "### Tensor Naming\n",
    "\n",
    "Each TensorFlow tensor has a unique name, and TensorBoard presents information under these names.\n",
    "\n",
    "+ __Name Scopes and Variable Scopes__: Use scoping to group tensors by adding a prefix to their names.\n",
    "+ __Name Method__: Use the argument `name=\"new_name\"` when defining a Edward random variable or TensorFlow tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "Run TensorBoard:\n",
    "```bash\n",
    "tensorboard --logdir=log/\n",
    "```\n",
    "\n",
    "Then go to the linked address. By default, it is http://localhost:6006. \n",
    "Here are some screenshots of what you should see.\n",
    "\n",
    "**TensorBoard Scalars**\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard_scalars.png)\n",
    "\n",
    "While the model is running you will see the variables values update in the graphs in the TensorBoard Scalars Tab.  You will see timeseries of the scalar variables in your model as well as other inference parameters relating to the specific loss functions and gradients used.\n",
    "\n",
    "**TensorBoard Distributions**\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard_distributions.png)\n",
    "\n",
    "The Distributions Tab will show how distributions of all of the values for a given variable over time.\n",
    "\n",
    "**TensorBoard Histograms**\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard_histograms.png)\n",
    "\n",
    "The Histograms is like the Distributions Tab except it shows the distributions over time as a three-dimensional chart.\n",
    "\n",
    "**TensorBoard Graphs**\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard_graphs.png)\n",
    "\n",
    "By cleaning up the variable naming as above you get a really clean image of the models in tensorflow.  Here you can see the default view that groups variables by their ``name_scope``.\n",
    "\n",
    "**TensorBoard Graphs (Close-Up)**\n",
    "![](https://raw.githubusercontent.com/blei-lab/edward/master/docs/images/tensorboard_graph1.png)\n",
    "\n",
    "By clicking into the nodes in the graph, you can see the detailed model structure. The more you name variables using custom names and use ``/`` to create a hierarchy in your naming convention, the cleaner this graphic will look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Supervised Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Simulate training and test sets of $40$ data points. They comprise of\n",
    "pairs of inputs $\\mathbf{x}_n\\in\\mathbb{R}^{5}$ and outputs\n",
    "$y_n\\in\\mathbb{R}$. They have a linear dependence with normally\n",
    "distributed noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w):\n",
    "  D = len(w)\n",
    "  x = np.random.normal(0.0, 2.0, size=(N, D))\n",
    "  y = np.dot(x, w) + np.random.normal(0.0, 0.01, size=N)\n",
    "  return x, y\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 5  # number of features\n",
    "\n",
    "# Variable scope adds this prefix to all data regardless if keyword 'name' is set\n",
    "with tf.variable_scope('data'): \n",
    "  w_true = np.random.randn(D) * 0.5\n",
    "  X_train, y_train = build_toy_dataset(N, w_true)\n",
    "  X_test, y_test = build_toy_dataset(N, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Posit the model as Bayesian linear regression (Murphy, 2012).\n",
    "For a set of $N$ data points $(\\mathbf{X},\\mathbf{y})=\\{(\\mathbf{x}_n, y_n)\\}$,\n",
    "the model posits the following distributions:\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{w})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\sigma_w^2\\mathbf{I}),\n",
    "  \\\\[1.5ex]\n",
    "  p(b)\n",
    "  &=\n",
    "  \\text{Normal}(b \\mid 0, \\sigma_b^2),\n",
    "  \\\\\n",
    "  p(\\mathbf{y} \\mid \\mathbf{w}, b, \\mathbf{X})\n",
    "  &=\n",
    "  \\prod_{n=1}^N\n",
    "  \\text{Normal}(y_n \\mid \\mathbf{x}_n^\\top\\mathbf{w} + b, \\sigma_y^2).\n",
    "\\end{align*}\n",
    "\n",
    "The latent variables are the linear model's weights $\\mathbf{w}$ and\n",
    "intercept $b$, also known as the bias.\n",
    "Assume $\\sigma_w^2,\\sigma_b^2$ are known prior variances and $\\sigma_y^2$ is a\n",
    "known likelihood variance. The mean of the likelihood is given by a\n",
    "linear transformation of the inputs $\\mathbf{x}_n$.\n",
    "\n",
    "Let's build the model in Edward, fixing $\\sigma_w,\\sigma_b,\\sigma_y=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name scope only adds this prefix to names where keyword 'name' is set\n",
    "with tf.name_scope('model'): \n",
    "  X = tf.placeholder(tf.float32, [N, D], name=\"X\")\n",
    "  w = Normal(loc=tf.zeros(D, name=\"weights/loc\"), scale=tf.ones(D, name=\"weights/loc\"), name=\"weights\")\n",
    "  b = Normal(loc=tf.zeros(1, name=\"bias/loc\"), scale=tf.ones(1, name=\"bias/scale\"), name=\"bias\")\n",
    "  y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N, name=\"y/scale\"), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a placeholder `X`. During inference, we pass in\n",
    "the value for this placeholder according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We now turn to inferring the posterior using variational inference.\n",
    "Define the variational model to be a fully factorized normal across\n",
    "the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d73b8e86bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'posterior'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     qw = Normal(loc=tf.Variable(tf.random_normal([D]), name=\"qw/loc\"),\n\u001b[0m\u001b[1;32m      3\u001b[0m                 scale=tf.nn.softplus(tf.Variable(tf.random_normal([D])), name=\"qw/scale\"), name=\"qw\")\n\u001b[1;32m      4\u001b[0m     qb = Normal(loc=tf.Variable(tf.random_normal([1]), name=\"qb/loc\"),\n\u001b[1;32m      5\u001b[0m                 scale=tf.nn.softplus(tf.Variable(tf.random_normal([1])), name=\"qb/scale\"), name=\"qb\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('posterior'):\n",
    "  qw = Normal(loc=tf.Variable(tf.random_normal([D]), name=\"qw/loc\"),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([D])), name=\"qw/scale\"), name=\"qw\")\n",
    "  qb = Normal(loc=tf.Variable(tf.random_normal([1]), name=\"qb/loc\"),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([1])), name=\"qb/scale\"), name=\"qb\")\n",
    "\n",
    "# Optionally create an 'inference' name_scope.  \n",
    "# If it is absent, the charts are grouped nicely by 'parameters', 'gradient_norm' and 'loss'\n",
    "# If it is added, TensorBoard Graph is slightly more organized.\n",
    "inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})   \n",
    "inference.run(n_samples=5, n_iter=250, logdir='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "We thank Sean Kruzel for writing the initial version of this\n",
    "tutorial.\n",
    "\n",
    "A TensorFlow tutorial to TensorBoard can be found \n",
    "[here](https://www.tensorflow.org/get_started/summaries_and_tensorboard)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
